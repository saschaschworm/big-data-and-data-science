{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started with demand forecasting, click [here](https://colab.research.google.com/github/saschaschworm/big-data-and-data-science/blob/master/labs/demand-forecasting.ipynb) to open the Jupyter Notebook in your Google Colab account. This notebook contains the exercises and code you can work through directly in Colab. If youâ€™d like to check your work or explore the completed solution, click [here](https://colab.research.google.com/github/saschaschworm/big-data-and-data-science/blob/master/labs/demand-forecasting-solution.ipynb) to import the solution notebook into Google Colab. Both resources are designed to help you deepen your understanding of demand forecasting and apply data science techniques hands-on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FreshMart, a local supermarket, struggles with managing the inventory of freshly prepared salads, which must be sold the same day they are delivered due to a one-day shelf life. The current ordering method, based on demand from exactly seven days ago plus a 10 % buffer, often results in either overstocking, leading to waste, or understocking, causing missed sales. To address these issues, FreshMart has initiated a data-driven project to improve inventory management.\n",
    "\n",
    "### Business Objectives\n",
    "\n",
    "The project's main objective is to **reduce salad waste while ensuring sufficient stock to meet demand**, balancing waste reduction with product availability to improve inventory efficiency, profitability, and customer satisfaction.\n",
    "\n",
    "### Situtation Assessment\n",
    "\n",
    "Currently, FreshMart uses a simple but flawed method to order salads, which doesn't account for daily demand fluctuations. The market has valuable resources like historical sales data but faces challenges such as the **salads' short shelf life**, **seasonal demand variations**, and **limited storage**. The risks include financial loss from **waste due to overestimation** and missed sales from **stockouts due to underestimation**.\n",
    "\n",
    "### Data Mining Goals\n",
    "\n",
    "The goal of the data mining process is to **develop a predictive model to accurately forecast daily salad demand**, enabling more informed ordering decisions. This model aims to create a dynamic system that provides **real-time order recommendations**, **reducing waste** and **improving stock availability**.\n",
    "\n",
    "### Project Plan\n",
    "\n",
    "The project is divided into five phases: data collection and preparation, exploratory data analysis and feature engineering, model development and testing, implementation and integration, and monitoring and optimization. These phases will ensure the chosen model is integrated into the inventory management system, with a focus on continuous performance tracking and adjustment.\n",
    "\n",
    "### Initial Assessment of Tools and Techniques\n",
    "\n",
    "Linear regression will be the primary modeling technique, with Python used for data analysis, modeling, visualization, and reporting. The data is available in CSV or Feather formats, eliminating the need for SQL databases. The model's performance will be evaluated using Root Mean Square Error (RMSE) and business metrics like waste reduction and stock turn rate improvement.\n",
    "\n",
    "### Success Criteria Determination\n",
    "\n",
    "Success will be measured by both business and technical criteria. Business success involves **reducing salad waste by at least 70 %**, **increasing profitability 80 %**, and **maintaining at least 50 %** in-stock rate. On the technical side, success will be determined by achieving at least **20** RMSE, and providing real-time order recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup, Configuration, and Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This installs the necessary libraries. If you're not working in Google Colab, alternatives like Poetry may also work \n",
    "# for managing dependencies. You can refer to the pyproject.toml in the GitHub repository for more information.\n",
    "#\n",
    "# !pip install -q feature_engine pyampute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third-Party Packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "\n",
    "from feature_engine.pipeline import make_pipeline\n",
    "from feature_engine.wrappers import SklearnTransformerWrapper\n",
    "\n",
    "from holidays import country_holidays\n",
    "\n",
    "from matplotlib.dates import DateFormatter\n",
    "from matplotlib.ticker import FixedFormatter, FixedLocator, MultipleLocator, PercentFormatter\n",
    "\n",
    "from pandas import DataFrame\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV, TimeSeriesSplit, cross_validate, train_test_split\n",
    "from sklearn.preprocessing import FunctionTransformer, PolynomialFeatures, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import data_table\n",
    "    data_table.enable_dataframe_formatter()\n",
    "except ModuleNotFoundError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for Matplotlib and Seaborn\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "# Configuration for Pandas\n",
    "pd.set_option(\"display.max_rows\", 11)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.float_format\", lambda x: f\"{x:,.4f}\")\n",
    "\n",
    "# Configuration for Scikit-Learn\n",
    "sklearn.set_config(transform_output=\"pandas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_business_metrics(transactions: DataFrame):\n",
    "    \"\"\"Calculate key business metrics from transaction data.\n",
    "\n",
    "    This function computes various key performance indicators (KPIs) related to inventory and sales based on the \n",
    "    provided transaction data.\n",
    "\n",
    "    Args:\n",
    "        transactions (DataFrame): A data frame with the following required columns: actual demand for the product \n",
    "            (`DEMAND`), purchased quantity of the product (`PQTY`), cost of excess inventory (`CEI`), cost of lost \n",
    "            sales (`CLS`), sales revenue (`SR`), purchase cost (`PC`), gross profit (`GP`), net profit (`NP`), and \n",
    "            inventory levels (`INVENTORY`).\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: A data frame containing the calculated KPIs with the following columns: the name of the key \n",
    "            performance indicator (`KPI`), a brief description of the KPI (`DESCRIPTION`), and the calculated value of \n",
    "            the KPI, formatted as a string (`VALUE`).\n",
    "\n",
    "    Raises:\n",
    "        KeyError: If required columns are missing in the input data frame.\n",
    "    \"\"\"\n",
    "\n",
    "    metrics = DataFrame(columns=[\"KPI\", \"DESCRIPTION\", \"VALUE\"])\n",
    "    metrics.loc[len(metrics), :] = [\"Root Mean Squared Error\", \"Measures the average difference between actual demand and purchased quantity.\", f\"{root_mean_squared_error(transactions['DEMAND'], transactions['PQTY']):,.2f} UNITS\"]\n",
    "    metrics.loc[len(metrics), :] = [\"Total Cost of Inefficiency\", \"The combined cost of excess inventory and lost sales.\", f\"{(transactions['CEI'] + transactions['CLS']).sum():,.2f} EUR\"]\n",
    "    metrics.loc[len(metrics), :] = [\"Total Sales Revenue\", \"Total revenue generated from sales.\", f\"{transactions['SR'].sum():,.2f} EUR\"]\n",
    "    metrics.loc[len(metrics), :] = [\"Total Purchase Cost\", \"Total cost incurred from purchasing goods.\", f\"{transactions['PC'].sum():,.2f} EUR\"]\n",
    "    metrics.loc[len(metrics), :] = [\"Total Gross Profit\", \"Total profit before accounting for overhead costs.\", f\"{transactions['GP'].sum():,.2f} EUR\"]\n",
    "    metrics.loc[len(metrics), :] = [\"Total Net Profit\", \"Total profit after all expenses have been deducted.\", f\"{transactions['NP'].sum():,.2f} EUR\"]\n",
    "    metrics.loc[len(metrics), :] = [\"Total Waste Rate\", \"Percentage of inventory left over (waste) compared to total purchased quantity.\", f\"{np.abs(transactions[transactions['INVENTORY'] > 0]['INVENTORY'].sum()) / sum(transactions['PQTY']) * 100:,.2f} %\"]\n",
    "    metrics.loc[len(metrics), :] = [\"Total Waste Quantity\", \"Total number of units left as waste.\", f\"{np.abs(transactions[transactions['INVENTORY'] > 0]['INVENTORY'].sum())} UNITS\"]\n",
    "    metrics.loc[len(metrics), :] = [\"Overstock Rate\", \"Percentage of records where there is excess inventory.\", f\"{len(transactions[transactions['INVENTORY'] > 0]) / len(transactions) * 100:,.2f} %\"]\n",
    "    metrics.loc[len(metrics), :] = [\"Understock Rate\", \"Percentage of records where there is a shortage of inventory.\", f\"{len(transactions[transactions['INVENTORY'] < 0]) / len(transactions) * 100:,.2f} %\"]\n",
    "    metrics.loc[len(metrics), :] = [\"Perfect Order Rate\", \"Percentage of records where the inventory matched demand perfectly.\", f\"{len(transactions[transactions['INVENTORY'] == 0]) / len(transactions) * 100:,.2f} %\"]\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inventory_metrics(transactions: DataFrame, target = \"DEMAND\"):\n",
    "    \"\"\"Calculate detailed inventory and financial metrics from transaction data.\n",
    "\n",
    "    This function computes key financial metrics related to inventory, such as purchase cost, gross profit, inventory \n",
    "    levels, cost of excess inventory, cost of lost sales, and net profit, based on the provided transaction data.\n",
    "\n",
    "    Args:\n",
    "        transactions (DataFrame): A data frame containing the following required columns: purchase price per unit of \n",
    "            the product (`PPRC`), purchased quantity of the product (`PQTY`), actual demand for the product (`DEMAND`), \n",
    "            unit gross profit (profit per unsatisfied demand) (`UGP`).\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A data frame containing the calculated metrics with the following columns: total purchase cost \n",
    "            (`PC`), gross profit (`GP`), inventory levels (`INVENTORY`), cost of excess inventory (`CEI`), cost of lost \n",
    "            sales (`CLS`), net profit (`NP`), sales revenue for the product (`SR`), sales quantity (`SQTY`).\n",
    "\n",
    "    Raises:\n",
    "        KeyError: If required columns are missing in the input data frame.\n",
    "    \"\"\"\n",
    "\n",
    "    metrics = DataFrame(columns=[\"PC\", \"GP\", \"INVENTORY\", \"CEI\", \"CLS\", \"NP\", \"SQTY\", \"SR\"], index=transactions.index)\n",
    "    metrics[\"PC\"] = (transactions[\"PPRC\"] * transactions[\"PQTY\"]).round(2)\n",
    "    metrics[\"SQTY\"] = transactions[target].case_when([(lambda demand: demand >= transactions[\"PQTY\"], transactions[\"PQTY\"]), (lambda demand: demand < transactions[\"PQTY\"], transactions[\"DEMAND\"])])\n",
    "    metrics[\"SR\"] = (transactions[\"RPRC\"] * metrics[\"SQTY\"]).round(2)\n",
    "    metrics[\"GP\"] = (metrics[\"SR\"] - metrics[\"PC\"]).round(2)\n",
    "    metrics[\"INVENTORY\"] = transactions[\"PQTY\"] - transactions[\"DEMAND\"]\n",
    "    metrics[\"CEI\"] = metrics[\"INVENTORY\"].case_when([(lambda x: x > 0, metrics[\"INVENTORY\"] * transactions[\"PPRC\"]), (lambda x: x <= 0, 0)]).round(2)\n",
    "    metrics[\"CLS\"] = metrics[\"INVENTORY\"].case_when([(lambda x: x < 0,  -1 * metrics[\"INVENTORY\"] * transactions[\"UGP\"]), (lambda x: x >= 0, 0)]).round(2)\n",
    "    metrics[\"NP\"] = (metrics[\"GP\"] - metrics[\"CLS\"]).round(2)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Data Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ForwardFillTransformer = FunctionTransformer(lambda x: x.copy().ffill())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This phase involves getting acquainted with the data, identifying quality issues, and uncovering insights to guide the project's next steps. This phase typically starts with collecting relevant data from databases, files, or external sources. The data is then summarized to understand its structure, including variable types (categorical or numerical), record counts, and basic statistics like mean, median, standard deviation, and distributions. Exploratory Data Analysis (EDA) follows, using visualizations like histograms, scatter plots, and box plots to reveal patterns, trends, and relationships. Finally, data quality is assessed by identifying issues such as missing values, outliers, and inconsistencies, which are crucial to address in the Data Preparation phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historic Retail Transactions Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = pd.read_feather(\"https://github.com/saschaschworm/big-data-and-data-science/raw/refs/heads/master/datasets/demand-forecasting.feather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Field Name**  | **Data Type** | **Description**                                                                                                     |\n",
    "|-----------------|---------------|---------------------------------------------------------------------------------------------------------------------|\n",
    "| `ODATE`         | `str`         | The order date when all relevant information becomes available to predict the demand on the target date (`TDATE`). |\n",
    "| `TDATE`         | `str`         | The target date for which the demand and other metrics are being predicted. This is the actual date for which the forecast is applicable. |\n",
    "| `SEASON`        | `str`         | The season (e.g., Winter, Spring, Summer, Fall) at the target date (`TDATE`). |\n",
    "| `DEMAND`        | `int`         | The actual number of units demanded or sold on the target date (`TDATE`). This is the realized demand and serves as the ground truth for evaluating the accuracy of the forecast. |\n",
    "| `DEMAND7CD`     | `int`         | The actual number of units demanded or sold 7 calendar days before the target date (`TDATE`). This metric reflects the realized demand on week prior to the target date. |\n",
    "| `MARKETING`     | `str`         | The level of marketing effort deployed on the target date (`TDATE`). This is typically categorized as `LOW`, `MEDIUM`, or `HIGH`, and represents the intensity of marketing campaigns, advertisements, or promotions aimed at driving sales. |\n",
    "| `PROMOTION`     | `str`         | The type of promotion applied on the target date (`TDATE`), such as `NONE`, `DISCOUNT`, or `BOGO`. Promotions can have a significant impact on demand by incentivizing purchases. |\n",
    "| `CAF`           | `str`         | An indicator of whether a competitor is currently running an active marketing campaign on the target date (`TDATE`). This is a binary field with values `YES` or `NO`, where `YES` indicates that a competitor has an active campaign, which could potentially impact demand. |\n",
    "| `TEMPERATURE`   | `float`       | The forecasted temperature (in degrees Celsius) expected on the target date (`TDATE`), as predicted on the prediction date (`ODATE`). Weather conditions can influence consumer behavior, especially for certain seasonal products. |\n",
    "| `PRECIPITATION` | `float`       | The forecasted probability of precipitation expected on the target date (`TDATE`), as predicted on the prediction date (`ODATE`). This metric represents the likelihood of rain or other forms of precipitation, which can affect foot traffic and overall demand. |\n",
    "| `PPRC`          | `float`       | The purchase price per unit on the target date (`TDATE`). This is the cost at which the supermarket acquires each unit of the product from suppliers, and it directly influences the cost of goods sold (COGS). |\n",
    "| `RPRC`          | `float`       | The sale price per unit on the target date (`TDATE`). This is the price at which the supermarket sells each unit to customers. It is a crucial factor in determining revenue and profitability. |\n",
    "| `PQTY`          | `int`         | The quantity of units purchased on the target date (`TDATE`). This reflects the inventory replenishment made by the supermarket to meet anticipated demand. |\n",
    "| `SQTY`          | `int`         | The quantity of units sold on the target date (`TDATE`). This is the actual sales volume achieved on the target date and is used to calculate revenue and inventory levels. |\n",
    "| `INVENTORY`     | `int`         | The stock level on the target date (`TDATE`) after accounting for sales and purchases. A positive value indicates surplus inventory (overstocking), while a negative value indicates a shortage (understocking). Inventory management is critical for minimizing holding costs and avoiding lost sales. |\n",
    "| `UGP`           | `float`       | The unit gross profit on the target date (`TDATE`), calculated as the difference between the sale price (`RPRC`) and the purchase price (`PPRC`). This metric indicates the profit margin per unit sold. |\n",
    "| `SR`            | `float`       | The total sales revenue on the target date (`TDATE`), calculated as the product of the quantity sold (`SQTY`) and the sale price (`RPRC`). Sales revenue is a key indicator of the supermarkets's top-line performance. |\n",
    "| `PC`            | `float`       | The total purchase cost on the target date (`TDATE`), calculated as the product of the quantity purchased (`PQTY`) and the purchase price (`PPRC`). This represents the total expenditure on acquiring inventory. |\n",
    "| `GP`            | `float`       | The gross profit on the target date (`TDATE`), calculated as the difference between total sales revenue (`SR`) and total purchase cost (`PC`). Gross profit indicates the profitability before accounting for operating expenses. |\n",
    "| `CEI`           | `float`       | The cost of excessive inventory on the target date (`TDATE`). This represents the holding costs incurred due to overstocking. Holding costs can include storage, insurance, spoilage, and depreciation of unsold goods. It is calculated based on the excess units in inventory that exceed expected demand. High CEI indicates inefficient inventory management and can negatively impact profitability. |\n",
    "| `CLS`           | `float`       | The cost of lost sales due to understocking on the target date (`TDATE`). This cost represents the opportunity loss when demand exceeds available inventory, leading to missed sales opportunities. Understocking can result in dissatisfied customers and potential loss of market share. The cost is typically estimated based on the profit that could have been earned if sufficient inventory had been available. |\n",
    "| `NP`            | `float`       | The net profit on the target date (`TDATE`), calculated as the difference between gross profit (`GP`) and the cost of lost sales (`CLS`). Net profit is a key measure of overall profitability after accounting for all relevant costs, including inefficiencies in inventory management. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preliminary Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions.dtypes.to_frame().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_business_metrics(transactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Line Plot: Transaction Date and Demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scatter Plot: Retail Price and Demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = sns.scatterplot(x=\"RPRC\", y=\"DEMAND\", data=transactions)\n",
    "\n",
    "ax.set_xlabel(\"Retail Price\")\n",
    "ax.set_ylabel(\"Demand (Units)\")\n",
    "# ax.set_ylim(0, 500)\n",
    "\n",
    "# ax.xaxis.set_major_locator(MultipleLocator(0.5))\n",
    "# ax.xaxis.set_major_formatter(\"{x:.2f} â‚¬\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scatter Plot: Precipitation, Competitor Activity, and Demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = sns.scatterplot(x=\"PRECIPITATION\", y=\"DEMAND\", hue=\"CAF\", data=transactions, linewidth=1, alpha=1.0)\n",
    "\n",
    "ax.set_xlabel(\"Precipitation Probability\")\n",
    "ax.set_ylabel(\"Demand (Units)\")\n",
    "\n",
    "# ax.xaxis.set_major_formatter(PercentFormatter(xmax=1))\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Box and Whisker Plot: Season and Demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = sns.boxplot(x=\"SEASON\", y=\"DEMAND\", data=transactions)\n",
    "\n",
    "ax.set_xlabel(\"Seasonal Cycle\")\n",
    "ax.set_ylabel(\"Demand (Units)\")\n",
    "\n",
    "# ax.xaxis.set_major_locator(FixedLocator([0, 1, 2, 3]))\n",
    "# ax.xaxis.set_major_formatter(FixedFormatter([\"Fall\", \"Spring\", \"Summer\", \"Winter\"]))\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scatter Plot: Forecasted Temperature and Demand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Create a scatter plot to analyze the relationship between temperature and demand. What trends can you identify in how temperature affects demand?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "\n",
    "# <PLACE YOUR CODE HERE>\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Box and Whisker Plot: Promotion and Demand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Create a box plot to compare the effectiveness of different promotion types on demand, and identify which promotion type leads to the highest demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "\n",
    "# <PLACE YOUR CODE HERE>\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Box and Whisker Plot: Competitor Activity, Marketing, and Demand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Create a box plot to visualize demand based on competitor activity, using marketing intensitiy as the hue. When should marketing strategies be adjusted based on competitor activity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "\n",
    "# <PLACE YOUR CODE HERE>\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nutritional Trends Index Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nti = pd.read_csv(\"https://github.com/saschaschworm/big-data-and-data-science/raw/refs/heads/master/datasets/nutritional-trends-index.csv\", parse_dates=[\"RELEASE\", \"VALID_FROM\", \"VALID_TO\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Field Name** | **Data Type** | **Description**                                                                                                                                  |\n",
    "|----------------|---------------|--------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| `RELEASE`      | `str`         | The release date of the Nutritional Trends Index (NTI) data. This is the date on which the NTI value is published, indicating when the data becomes available. |\n",
    "| `TARGET`       | `str`         | The month and year for which the NTI is applicable. This field typically contains the abbreviated month name (e.g., \"SEP\" for September) and refers to the time period that the NTI value represents. |\n",
    "| `TYPE`         | `str`         | The type of NTI value provided, which is always `ACTUAL`, and refers to the final NTI value observed or recorded for the target month.           |                                                                            \n",
    "| `VALUE`        | `float`       | The NTI for the specified target month (`TARGET`). This index tracks shifts in consumer preferences, dietary habits, and nutritional awareness over time. |\n",
    "| `VALID_FROM`   | `str`         | The start date from which the NTI value is considered valid.                                                                                     |\n",
    "| `VALID_TO`     | `str`         | The end date up to which the NTI value is valid.                                                                                                 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preliminary Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nti.dtypes.to_frame().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nti.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Histogram: Nutritional Trends Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = sns.histplot(x=\"VALUE\", binwidth=1, data=nti)\n",
    "\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.set_xlabel(\"Nutritional Trends Index\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Line Plot: Nutritional Trends Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Create a line plot to examine the Nutritional Trends Index over time by date. At what time do unusual observations occur?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "\n",
    "# <PLACE YOUR CODE HERE>\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consumer Climate Index Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cci = pd.read_csv(\"https://github.com/saschaschworm/big-data-and-data-science/raw/refs/heads/master/datasets/consumer-climate-index.csv\", parse_dates=[\"RELEASE\", \"VALID_FROM\", \"VALID_TO\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Field Name** | **Data Type** | **Description**                                                                                                     |\n",
    "|----------------|---------------|---------------------------------------------------------------------------------------------------------------------|\n",
    "| `RELEASE`      | `str`         | The release date of the Consumer Climate Index (CCI) data. This is the date on which the forecast or actual value of the CCI is published. It indicates when the data becomes available to the public or businesses for decision-making. |\n",
    "| `TARGET`       | `str`         | The month and year for which the CCI is applicable. This field typically contains the abbreviated month name (e.g., \"SEP\" for September) and refers to the time period that the CCI value represents. |\n",
    "| `TYPE`         | `str`         | The type of CCI value provided, which can be either `FORECAST` or `ACTUAL`. `FORECAST` indicates a predicted CCI value released before or at the beginning of the target month. `ACTUAL` refers to the final CCI value observed or recorded for the target month. |\n",
    "| `VALUE`        | `float`       | The value of the CCI for the specified target month (`TARGET`). The CCI is a measure of consumer confidence and sentiment, with negative values indicating pessimism (reduced consumer spending and economic activity) and positive values indicating optimism (increased consumer spending and economic activity). The value is typically expressed as an index relative to a base period. |\n",
    "| `VALID_FROM`   | `str`         | The start date from which the CCI value is considered valid. For `FORECAST` data, this is typically the first day of the target month (`TARGET`). For `ACTUAL` data, it is the release date (`RELEASE`). |\n",
    "| `VALID_TO`     | `str`         | The end date up to which the CCI value is valid. For `FORECAST` data, this is typically the last day before the next forecast or actual value is released. For `ACTUAL` data, it is the last day of the target month (`TARGET`). This period defines the coverage of the CCI value in terms of its relevance to economic conditions. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preliminary Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cci.dtypes.to_frame().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cci.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### German Public Holidays Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays = DataFrame(country_holidays(country=\"DE\", subdiv=\"NW\", years=[2023, 2024]).items(), columns=[\"DATE\", \"HOLIDAY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Field Name** | **Data Type** | **Description**                                                                                                     |\n",
    "|----------------|---------------|---------------------------------------------------------------------------------------------------------------------|\n",
    "| `DATE`         | `str`         | The date of the public holiday in Germany. This field represents the specific day on which the holiday is observed. The date is formatted as `YYYY-MM-DD`. |\n",
    "| `HOLIDAY`      | `str`         | The name of the public holiday in Germany. This field contains the official name of the holiday in German, such as \"Neujahr\" (New Year's Day) or \"Tag der Deutschen Einheit\" (German Unity Day). |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preliminary Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays.dtypes.to_frame().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation and Evaluation for Univariate Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This phase transforms raw data into a format suitable for modeling, often making it the most time-consuming part of the process. It begins with data integration, where data from multiple sources are combined, such as merging datasets or joining tables. Next, data cleaning addresses issues identified during the previous phase. This includes handling missing values through imputation or removal, correcting outliers, and standardizing formats to resolve inconsistencies. Data transformation follows, performing feature engineering to create new, potentially more predictive features, and modifying the data for modeling by normalizing or standardizing numerical features, encoding categorical variables. Finally, data formatting ensures the data is properly structured and formatted for the modeling tools, including converting data types and ensuring consistent column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations = make_pipeline(\n",
    "    SklearnTransformerWrapper(ForwardFillTransformer, variables=[\"TEMPERATURE\"]),\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions[[\"TEMPERATURE\"]].transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions[[\"TEMPERATURE\"]].describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations.fit_transform(transactions[[\"TEMPERATURE\"]]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations.fit_transform(transactions[[\"TEMPERATURE\"]]).describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This phase involves selecting and applying algorithms to the prepared data to create predictive models. This phase requires careful selection of model types and parameters, as different models may be better suited to the specific data and problem. Multiple models are typically built and tested to identify the best performer. The process includes splitting the data into training and testing sets, training the models on the training data, and fine-tuning them by adjusting hyperparameters to optimize performance. The goal is to develop a model that accurately captures underlying patterns and generalizes well to new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(\n",
    "    transformations,\n",
    "    SGDRegressor(penalty=None, alpha=0.0001, max_iter=1000, tol=0.001, shuffle=True, verbose=0, random_state=1207, \n",
    "        learning_rate=\"invscaling\", eta0=0.001, power_t=0.25, n_iter_no_change=5,\n",
    "    ),\n",
    "    verbose=False,\n",
    ").fit(transactions[[\"TEMPERATURE\"]], transactions[\"DEMAND\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame([[model[-1].intercept_[0], *model[-1].coef_]], columns=[\"INTERCEPT\", \"TEMPERATURE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = transactions[[\"ODATE\", \"TDATE\", \"DEMAND\", \"TEMPERATURE\", \"PPRC\", \"RPRC\", \"UGP\"]].copy()\n",
    "predictions[\"YHAT\"] = model.predict(transactions[[\"TEMPERATURE\"]])\n",
    "predictions[\"PQTY\"] = np.ceil(predictions[\"YHAT\"])\n",
    "predictions = predictions.merge(get_inventory_metrics(predictions), left_index=True, right_index=True)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = DataFrame({\"TDATE\": pd.date_range(\"2024-09-01\", periods=7), \"TEMPERATURE\": [27, 28, 29, 28, 28, 27, 27]})\n",
    "forecast[\"FORECAST\"] = model.predict(forecast[[\"TEMPERATURE\"]])\n",
    "forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This phase assesses the performance of the models developed during the Modeling phase to ensure they meet the project's objectives. This involves testing the models on the test data and evaluating them with relevant metrics, such as accuracy, precision, recall, F1-score, or mean squared error, depending on the problem type (classification, regression, etc.). The process also includes validating the model's robustness and consistency across different data subsets, often using cross-validation techniques to ensure reliability. The goal is to identify the most effective model that strikes the best balance between performance and generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"RMSE (History): {root_mean_squared_error(transactions[\"DEMAND\"], transactions[\"PQTY\"]):,.2f} UNITS\")\n",
    "print(f\"RMSE (Model): {root_mean_squared_error(transactions[\"DEMAND\"], predictions[\"PQTY\"]):,.2f} UNITS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.merge(get_business_metrics(transactions), get_business_metrics(predictions), on=[\"KPI\", \"DESCRIPTION\"])\n",
    "metrics = metrics.rename({\"VALUE_x\": \"BENCHMARK\", \"VALUE_y\": \"SLR\"}, axis=1)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Holdout Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    transactions[[\"TEMPERATURE\"]], transactions[\"DEMAND\"], test_size=0.25, shuffle=True, random_state=1207,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.merge(y_train, left_index=True, right_index=True).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.merge(y_test, left_index=True, right_index=True).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"RMSE (Training Set): {root_mean_squared_error(y_train, model.predict(X_train)):,.2f} UNITS\")\n",
    "print(f\"RMSE (Test Set): {root_mean_squared_error(y_test, model.predict(X_test)):,.2f} UNITS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10, shuffle=False)\n",
    "evaluation = cross_validate(\n",
    "    estimator=model, X=transactions[[\"TEMPERATURE\"]], y=transactions[\"DEMAND\"], cv=kfold, n_jobs=-1, \n",
    "    scoring=\"neg_root_mean_squared_error\", verbose=0, return_train_score=True,\n",
    ")\n",
    "cv = DataFrame(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.agg(\"mean\").to_frame().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Conduct a $k$-fold cross-validation analysis while varying the value of k. What observations can you make regarding the modelâ€™s performance as you change $k$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time-Series Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=10, gap=1)\n",
    "evaluation = cross_validate(\n",
    "    estimator=model, X=transactions[[\"TEMPERATURE\"]], y=transactions[\"DEMAND\"], cv=tscv, n_jobs=-1, \n",
    "    scoring=\"neg_root_mean_squared_error\", verbose=0, return_train_score=True,\n",
    ")\n",
    "cv = DataFrame(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.agg(\"mean\").to_frame().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation, Modeling, and Evaluation (Part II)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(\n",
    "    SklearnTransformerWrapper(ForwardFillTransformer, variables=[\"TEMPERATURE\"]),\n",
    "    SklearnTransformerWrapper(PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    SGDRegressor(penalty=None, alpha=0.0001, max_iter=1000, tol=0.001, shuffle=True, verbose=0, random_state=1207, \n",
    "        learning_rate=\"invscaling\", eta0=0.001, power_t=0.25, n_iter_no_change=5,\n",
    "    ),\n",
    "    verbose=False,\n",
    ").fit(transactions[[\"TEMPERATURE\"]], transactions[\"DEMAND\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame([[model[-1].intercept_[0], *model[-1].coef_]], columns=[\"INTERCEPT\", *model[:-1].get_feature_names_out()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation, Modeling, and Evaluation (Part III)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(\n",
    "    SklearnTransformerWrapper(ForwardFillTransformer, variables=[\"TEMPERATURE\"]),\n",
    "    SklearnTransformerWrapper(PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    SklearnTransformerWrapper(StandardScaler()),\n",
    "    SGDRegressor(penalty=None, alpha=0.0001, max_iter=1000, tol=0.001, shuffle=True, verbose=0, random_state=1207, \n",
    "        learning_rate=\"invscaling\", eta0=0.1, power_t=0.25, n_iter_no_change=5,\n",
    "    ),\n",
    "    verbose=False,\n",
    ").fit(transactions[[\"TEMPERATURE\"]], transactions[\"DEMAND\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame(\n",
    "    data=[[model[-1].intercept_[0], *model[-1].coef_]],\n",
    "    columns=[\"INTERCEPT\", *model[:-1].get_feature_names_out()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=10, gap=1)\n",
    "evaluation = cross_validate(\n",
    "    estimator=model, X=transactions[[\"TEMPERATURE\"]], y=transactions[\"DEMAND\"], cv=tscv, n_jobs=-1, \n",
    "    scoring=\"neg_root_mean_squared_error\", verbose=0, return_train_score=True,\n",
    ")\n",
    "cv = DataFrame(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.agg(\"mean\").to_frame().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Experiment with different polynomial degrees to determine the best fit for your model. Then, apply a high polynomial degree while varying the L1 (`penalty = \"l1\"`) and L2 (`penalty = \"l2\"`) regularization rates. What do you notice about the coefficients as you change the regularization rate and learning rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation, Modeling, and Evaluation (Part IV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.randint(1, 11).rvs(size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"sklearntransformerwrapper-2__transformer__degree\": stats.randint(1, 11),\n",
    "    \"sgdregressor__alpha\": stats.loguniform(0.0001, 0.1),\n",
    "    \"sgdregressor__max_iter\": stats.randint(1000, 3001),\n",
    "    \"sgdregressor__eta0\": stats.loguniform(0.0001, 0.1),\n",
    "    \"sgdregressor__penalty\": [\"l2\", \"l1\", None],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(transactions[[\"TEMPERATURE\"]], transactions[\"DEMAND\"], shuffle=False)\n",
    "tscv = TimeSeriesSplit(n_splits=10, gap=1)\n",
    "\n",
    "evaluation = RandomizedSearchCV(\n",
    "    estimator=model, param_distributions=params, n_iter=10, scoring=\"neg_root_mean_squared_error\", cv=tscv, \n",
    "    random_state=1207, n_jobs=-1, return_train_score=True, verbose=0,\n",
    ")\n",
    "\n",
    "evaluation = evaluation.fit(X_train, y_train)\n",
    "cv = DataFrame(evaluation.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.drop(\"params\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv[[\"mean_train_score\", \"std_train_score\", \"mean_test_score\", \"std_test_score\"]].agg(\"mean\").to_frame().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame([evaluation.best_params_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.best_estimator_[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame(\n",
    "    data=[[evaluation.best_estimator_[-1].intercept_[0], *evaluation.best_estimator_[-1].coef_]], \n",
    "    columns=[\"INTERCEPT\", *evaluation.best_estimator_[:-1].get_feature_names_out()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = transactions[[\"ODATE\", \"TDATE\", \"DEMAND\", \"TEMPERATURE\", \"PPRC\", \"RPRC\", \"UGP\"]].copy()\n",
    "predictions[\"YHAT\"] = evaluation.best_estimator_.predict(transactions[[\"TEMPERATURE\"]])\n",
    "predictions[\"PQTY\"] = np.ceil(predictions[\"YHAT\"])\n",
    "predictions = predictions.merge(get_inventory_metrics(predictions), left_index=True, right_index=True)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.merge(get_business_metrics(transactions), get_business_metrics(predictions), on=[\"KPI\", \"DESCRIPTION\"])\n",
    "metrics = metrics.rename({\"VALUE_x\": \"BENCHMARK\", \"VALUE_y\": \"SLR\"}, axis=1)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = sns.scatterplot(x=\"TEMPERATURE\", y=\"DEMAND\", data=predictions, c=\"C0\", alpha=0.5)\n",
    "ax = sns.lineplot(x=\"TEMPERATURE\", y=\"YHAT\", data=predictions, c=\"C1\")\n",
    "\n",
    "ax.set_xlabel(\"Forecasted Temperature (Â°C)\")\n",
    "ax.set_ylabel(\"Demand (Units)\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = sns.lineplot(x=\"TDATE\", y=\"DEMAND\", data=predictions, c=\"C0\", alpha=0.5)\n",
    "ax = sns.lineplot(x=\"TDATE\", y=\"YHAT\", data=predictions, c=\"C1\")\n",
    "\n",
    "ax.set_xlabel(\"Transaction Date\")\n",
    "ax.set_ylabel(\"Demand (Units)\")\n",
    "\n",
    "ax.xaxis.set_major_formatter(DateFormatter(\"%b %d, %Y\"))\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation and Evaluation for Multivariate Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing the univariate analysis notebook, you can continue your exploration of demand forecasting by accessing the next notebook, which focuses on multivariate analysis. This upcoming notebook will introduce more sophisticated methods, including feature engineering and outlier handling, to enhance your analysis. Click [here](https://colab.research.google.com/github/saschaschworm/big-data-and-data-science/blob/master/labs/demand-forecasting-advanced.ipynb) to dive into these advanced techniques and further expand your understanding of data science in the context of demand forecasting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
